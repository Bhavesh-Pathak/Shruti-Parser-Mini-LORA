{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35645c3",
   "metadata": {},
   "source": [
    "# Mini LoRA Benchmark Kit\n",
    "\n",
    "This notebook demonstrates a tiny local LoRA fine-tuning workflow using `distilbert-base-uncased` on a 12-sample binary classification task, then benchmarks inference latency, token throughput, and quality (accuracy/F1) vs the base model.\n",
    "\n",
    "Assumptions: we use DistilBERT (encoder) for a sequence classification demonstration because it's small and locally runnable; this is a classification (not generative) benchmark, so 'token response speed' is interpreted as tokens processed per second during inference. If you want a generative benchmark (per-token generation latency), we can switch to a causal model in a follow-up (e.g., distilgpt2 or a small Mistral/GPT model) â€” tell me if you'd prefer that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once).\n",
    "# Uncomment and run the next cell in your environment if packages are not installed.\n",
    "# !pip install -r ../requirements.txt\n",
    "\n",
    "print('Skip install in notebook if running in prepared env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import torch\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cce18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mini dataset from CSV\n",
    "df = pd.read_csv('../data/mini_dataset.csv')\n",
    "df['label'] = df['label'].astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small train/test split (10 train / 2 test)\n",
    "train_df = df.sample(frac=0.83, random_state=42)  # ~10 samples\n",
    "test_df = df.drop(train_df.index)\n",
    "train = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3692ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and model init\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Base model (we'll clone this for comparing base vs LoRA)\n",
    "base_model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "def preprocess(batch):\n",
    "    toks = tokenizer(batch['text'], truncation=True, padding=False)\n",
    "    toks['labels'] = batch['label']\n",
    "    return toks\n",
    "\n",
    "train = train.map(preprocess, batched=True)\n",
    "test = test.map(preprocess, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "print('Tokenization done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a294ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (tiny)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./outputs',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    save_strategy='no',\n",
    "    disable_tqdm=False,\n",
    "    fp16=False\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(pred.label_ids, preds)\n",
    "    f1 = f1_score(pred.label_ids, preds, zero_division=0)\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "print('Training args and metric function prepared')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869135b",
   "metadata": {},
   "source": [
    "### 1) Evaluate base model performance and inference latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db0b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model (no further training)\n",
    "trainer_base = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "eval_base = trainer_base.evaluate()\n",
    "eval_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure inference latency and tokens/sec for the base model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model.to(device)\n",
    "\n",
    "def measure_latency_and_throughput(model, tokenizer, texts, device, repeat=20):\n",
    "    # Warmup\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    for _ in range(3):\n",
    "        _ = model(**inputs)\n",
    "    # Measure\n",
    "    t0 = time.time()\n",
    "    for _ in range(repeat):\n",
    "        _ = model(**inputs)\n",
    "    t_total = time.time() - t0\n",
    "    avg_latency = t_total / repeat\n",
    "    # tokens per second = total input tokens across batch / avg_latency\n",
    "    batch_token_count = sum([len(tokenizer(tok)['input_ids']) for tok in texts])\n",
    "    tokens_per_sec = batch_token_count / avg_latency\n",
    "    return avg_latency, tokens_per_sec\n",
    "\n",
    "sample_texts = test['text'] if len(test) > 0 else train['text'][:2]\n",
    "base_latency, base_tps = measure_latency_and_throughput(base_model, tokenizer, sample_texts, device)\n",
    "{'latency_s': base_latency, 'tokens_per_sec': base_tps}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f5922",
   "metadata": {},
   "source": [
    "### 2) Apply LoRA (PEFT) and fine-tune on the mini dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc0c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh base model copy to apply LoRA to (avoid modifying the earlier 'base_model' used for eval)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# LoRA config (very small for demo)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    ", \n",
    ", \n",
    ", \n",
    ", \n",
    ", \n",
    "],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\n",
    ",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "print('PEFT/LoRA model created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer for LoRA model\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune (very small; fast)\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the fine-tuned LoRA model\n",
    "eval_lora = trainer.evaluate()\n",
    "eval_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure LoRA model latency and tokens/sec\n",
    "peft_model.to(device)\n",
    "lora_latency, lora_tps = measure_latency_and_throughput(peft_model, tokenizer, sample_texts, device)\n",
    "{'latency_s': lora_latency, 'tokens_per_sec': lora_tps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74342610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outputs on test set (detailed)\n",
    "def get_preds(model, dataset):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        item = {k: torch.tensor([v]).to(device) for k, v in dataset[i].items() if k in ['input_ids','attention_mask']}\n",
    "        with torch.no_grad():\n",
    "            out = model(**item)\n",
    "            logits = out.logits.cpu().numpy()[0]\n",
    "            preds.append(int(np.argmax(logits)))\n",
    "            labels.append(int(dataset[i]['labels']))\n",
    "    return preds, labels\n",
    "\n",
    "base_model.to(device)\n",
    "base_preds, base_labels = get_preds(base_model, test)\n",
    "lora_preds, lora_labels = get_preds(peft_model, test)\n",
    "\n",
    "print(\"Base classification report:\", classification_report(base_labels, base_preds, zero_division=0))\n",
    "print('LoRA classification report:', classification_report(lora_labels, lora_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200bde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON and CSV\n",
    "results = {\n",
    "    'base_eval': eval_base,\n",
    "    'lora_eval': eval_lora,\n",
    "    'base_latency_s': base_latency,\n",
    "    'base_tokens_per_sec': base_tps,\n",
    "    'lora_latency_s': lora_latency,\n",
    "    'lora_tokens_per_sec': lora_tps,\n",
    "    'test_size': len(test)\n",
    "}\n",
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "with open('../results/results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "# Also produce a flat CSV summary\n",
    "pd.DataFrame([{\n",
    "    'accuracy': eval_base.get('eval_accuracy', None),\n",
    "    'latency_s': base_latency,\n",
    "    'accuracy': eval_lora.get('eval_accuracy', None),\n",
    "    'latency_s': lora_latency,\n",
    "}])\n",
    "print('Saved results to ../results/results.json and ../results/results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6f0a04",
   "metadata": {},
   "source": [
    "## Notes and limitations\n",
    "- This demo uses a tiny dataset (12 samples) and only demonstrates the pipeline. Results are not statistically meaningful.\n",
    "- Using DistilBERT for sequence classification shows how LoRA works on an encoder; for per-token generative latency (e.g., tokens/sec generated), use a causal LM (distilgpt2 or a small Mistral) in a follow-up.\n",
    "- LoRA hyperparameters here are small to keep training quick; adjust `r`, `lora_alpha`, and `target_modules` for your workloads."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
