{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b73aa2",
   "metadata": {},
   "source": [
    "# Mini LoRA Benchmark Kit (Colab Version)\n",
    "\n",
    "This notebook demonstrates a tiny local LoRA fine-tuning workflow using `distilbert-base-uncased` on a 12-sample binary classification task, then benchmarks inference latency, token throughput, and quality (accuracy/F1) vs the base model.\n",
    "\n",
    "First, we'll set up Google Drive and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceddc815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (needed to save results)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory\n",
    "!mkdir -p /content/mini_lora_benchmark\n",
    "!mkdir -p /content/mini_lora_benchmark/data\n",
    "!mkdir -p /content/mini_lora_benchmark/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e459186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers>=4.30.0 datasets>=2.8.0 peft>=0.3.0 accelerate>=0.20.0 scikit-learn>=1.1.0 pandas>=1.3.0 torch>=1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7b3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mini dataset\n",
    "mini_dataset = \"\"\"\n",
    "text,label\n",
    "\"I love this product, it works great!\",1\n",
    "\"Terrible quality, broke after a day.\",0\n",
    "\"Excellent value for money.\",1\n",
    "\"I wouldn't buy this again.\",0\n",
    "\"Very happy with the purchase.\",1\n",
    "\"It was okay, nothing special.\",0\n",
    "\"Exceeded my expectations.\",1\n",
    "\"Not what I expected, disappointing.\",0\n",
    "\"Five stars, highly recommend.\",1\n",
    "\"One star. Do not recommend.\",0\n",
    "\"Works fine for the price.\",1\n",
    "\"Poorly made and slow.\",0\n",
    "\"\"\"\n",
    "\n",
    "with open('/content/mini_lora_benchmark/data/mini_dataset.csv', 'w') as f:\n",
    "    f.write(mini_dataset.strip())\n",
    "\n",
    "print(\"Created dataset at /content/mini_lora_benchmark/data/mini_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4aaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import torch\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mini dataset from CSV\n",
    "df = pd.read_csv('/content/mini_lora_benchmark/data/mini_dataset.csv')\n",
    "df['label'] = df['label'].astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c02b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small train/test split (10 train / 2 test)\n",
    "train_df = df.sample(frac=0.83, random_state=42)  # ~10 samples\n",
    "test_df = df.drop(train_df.index)\n",
    "train = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd98d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and model init\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Base model (we'll clone this for comparing base vs LoRA)\n",
    "base_model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "def preprocess(batch):\n",
    "    toks = tokenizer(batch['text'], truncation=True, padding=False)\n",
    "    toks['labels'] = batch['label']\n",
    "    return toks\n",
    "\n",
    "train = train.map(preprocess, batched=True)\n",
    "test = test.map(preprocess, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "print('Tokenization done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (tiny)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/mini_lora_benchmark/outputs',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    save_strategy='no',\n",
    "    disable_tqdm=False,\n",
    "    fp16=torch.cuda.is_available()  # Enable fp16 if CUDA available\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(pred.label_ids, preds)\n",
    "    f1 = f1_score(pred.label_ids, preds, zero_division=0)\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "print('Training args and metric function prepared')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554db2a0",
   "metadata": {},
   "source": [
    "### 1) Evaluate base model performance and inference latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model (no further training)\n",
    "trainer_base = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "eval_base = trainer_base.evaluate()\n",
    "eval_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure inference latency and tokens/sec for the base model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model.to(device)\n",
    "\n",
    "def measure_latency_and_throughput(model, tokenizer, texts, device, repeat=20):\n",
    "    # ensure texts are Python strings\n",
    "    texts = [str(t) for t in texts]\n",
    "    # Warmup\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    for _ in range(3):\n",
    "        _ = model(**inputs)\n",
    "    # Measure\n",
    "    t0 = time.time()\n",
    "    for _ in range(repeat):\n",
    "        _ = model(**inputs)\n",
    "    t_total = time.time() - t0\n",
    "    avg_latency = t_total / repeat\n",
    "    # tokens per second = total input tokens across batch / avg_latency\n",
    "    batch_token_count = sum([len(tokenizer(tok)['input_ids']) for tok in texts])\n",
    "    tokens_per_sec = batch_token_count / avg_latency\n",
    "    return avg_latency, tokens_per_sec\n",
    "\n",
    "sample_texts = test['text'] if len(test) > 0 else train['text'][:2]\n",
    "base_latency, base_tps = measure_latency_and_throughput(base_model, tokenizer, sample_texts, device)\n",
    "{'latency_s': base_latency, 'tokens_per_sec': base_tps}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6a240",
   "metadata": {},
   "source": [
    "### 2) Apply LoRA (PEFT) and fine-tune on the mini dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411fee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh base model copy to apply LoRA to (avoid modifying the earlier 'base_model' used for eval)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# LoRA config (very small for demo)\n",
    "# DistilBERT attention uses q_lin/k_lin/v_lin naming\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"],  # DistilBERT attention layer names\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "print('PEFT/LoRA model created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dab365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer for LoRA model\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine-tune (very small; fast)\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the fine-tuned LoRA model\n",
    "eval_lora = trainer.evaluate()\n",
    "eval_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure LoRA model latency and tokens/sec\n",
    "peft_model.to(device)\n",
    "lora_latency, lora_tps = measure_latency_and_throughput(peft_model, tokenizer, sample_texts, device)\n",
    "{'latency_s': lora_latency, 'tokens_per_sec': lora_tps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outputs on test set (detailed)\n",
    "def get_preds(model, dataset):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        item = {k: torch.tensor([v]).to(device) for k, v in dataset[i].items() if k in ['input_ids','attention_mask']}\n",
    "        with torch.no_grad():\n",
    "            out = model(**item)\n",
    "            logits = out.logits.cpu().numpy()[0]\n",
    "            preds.append(int(np.argmax(logits)))\n",
    "            labels.append(int(dataset[i]['labels']))\n",
    "    return preds, labels\n",
    "\n",
    "base_model.to(device)\n",
    "base_preds, base_labels = get_preds(base_model, test)\n",
    "lora_preds, lora_labels = get_preds(peft_model, test)\n",
    "\n",
    "print(\"Base classification report:\\n\", classification_report(base_labels, base_preds, zero_division=0))\n",
    "print('\\nLoRA classification report:\\n', classification_report(lora_labels, lora_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON and CSV in Google Drive\n",
    "results = {\n",
    "    'base_eval': eval_base,\n",
    "    'lora_eval': eval_lora,\n",
    "    'base_latency_s': base_latency,\n",
    "    'base_tokens_per_sec': base_tps,\n",
    "    'lora_latency_s': lora_latency,\n",
    "    'lora_tokens_per_sec': lora_tps,\n",
    "    'test_size': len(test)\n",
    "}\n",
    "\n",
    "# Save to local results directory\n",
    "with open('/content/mini_lora_benchmark/results/results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Also produce a flat CSV summary\n",
    "pd.DataFrame([{\n",
    "    'model': 'base',\n",
    "    'accuracy': eval_base.get('eval_accuracy', None),\n",
    "    'f1': eval_base.get('eval_f1', None),\n",
    "    'latency_s': base_latency,\n",
    "    'tokens_per_sec': base_tps\n",
    "}, {\n",
    "    'model': 'lora',\n",
    "    'accuracy': eval_lora.get('eval_accuracy', None),\n",
    "    'f1': eval_lora.get('eval_f1', None),\n",
    "    'latency_s': lora_latency,\n",
    "    'tokens_per_sec': lora_tps\n",
    "}]).to_csv('/content/mini_lora_benchmark/results/results.csv', index=False)\n",
    "\n",
    "# Optional: Copy results to Google Drive\n",
    "!cp -r /content/mini_lora_benchmark/results/* /content/drive/MyDrive/mini_lora_benchmark/results/\n",
    "\n",
    "print('Saved results locally and to Google Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d930c9f",
   "metadata": {},
   "source": [
    "## Notes and limitations\n",
    "- This demo uses a tiny dataset (12 samples) and only demonstrates the pipeline. Results are not statistically meaningful.\n",
    "- Using DistilBERT for sequence classification shows how LoRA works on an encoder; for per-token generative latency (e.g., tokens/sec generated), use a causal LM (distilgpt2 or a small Mistral) in a follow-up.\n",
    "- LoRA hyperparameters here are small to keep training quick; adjust `r`, `lora_alpha`, and `target_modules` for your workloads.\n",
    "- In Colab, you'll likely get much better performance using the GPU runtime (Runtime → Change runtime type → GPU)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
